import torch
import torch.nn as nn
import torch.nn.functional as F

class FreeFlowFPN(nn.Module):
    def __init__(self, in_channels, out_channels, tau=0.1):
        """
        è‡ªç”±æµåŠ¨ FPN (Free-Flow Feature Pyramid Network)
        :param in_channels: æ¯ä¸ªç‰¹å¾å±‚çš„é€šé“æ•°
        :param out_channels: æ¯å±‚è¾“å‡ºçš„é€šé“æ•°
        :param tau: å‰ªæé˜ˆå€¼ï¼ˆä½äºè¯¥å€¼çš„è·¯å¾„ç›´æ¥å‰ªæ‰ï¼‰
        """
        super().__init__()
        self.tau = tau  # å‰ªæé˜ˆå€¼

        # è®¡ç®—å¯åŠ¨é¡ºåºçš„ MLP
        self.mlp = nn.Sequential(
            nn.Linear(in_channels, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # è¾“å‡ºé‡è¦æ€§åˆ†æ•°
        )

        # è®¡ç®— QKï¼Œç”¨äºè·¯å¾„æƒé‡
        self.qk_proj = nn.Linear(in_channels, in_channels * 2)

        # FPN ç»†åŒ–å·ç§¯å±‚ (3x3 å·ç§¯)
        self.fpn_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1) for _ in range(4)
        ])

    def forward(self, features):
        batch_size, channels, _, _ = features[0].shape
        num_layers = len(features)

        # è®¡ç®—æ¯å±‚çš„é‡è¦æ€§åˆ†æ•°
        feature_vecs = [F.adaptive_avg_pool2d(f, (1, 1)).view(batch_size, -1) for f in features]
        feature_tensor = torch.stack(feature_vecs, dim=1)
        scores = self.mlp(feature_tensor).squeeze(-1)
        order = torch.argsort(scores, dim=1, descending=True)  # æŒ‰é‡è¦æ€§é™åºæ’åº

        # è®¡ç®— QK æ³¨æ„åŠ›
        qk = self.qk_proj(feature_tensor)
        q, k = torch.chunk(qk, 2, dim=-1)
        attn = torch.matmul(q, k.transpose(-2, -1)) / (channels ** 0.5)

        # ğŸš¨ **å±è”½è‡ªèº«æƒé‡ï¼ˆåœ¨ softmax ä¹‹å‰ï¼ï¼‰**
        attn[:, torch.arange(num_layers), torch.arange(num_layers)] = float('-inf')

        attn = F.softmax(attn, dim=-1)  # å½’ä¸€åŒ–

        # 3ï¸âƒ£ è®¡ç®—æ–°çš„ç‰¹å¾å›¾ï¼ˆä¿¡æ¯èåˆï¼‰
        new_features = [features[i].clone() for i in range(num_layers)]  # å…ˆä¿ç•™åŸå§‹ç‰¹å¾

        # **æ‰€æœ‰å±‚éƒ½å¯ä»¥å‘å…¶ä»–å±‚ä¼ æ’­**
        for i in range(num_layers):  # **æŒ‰ç…§ order é¡ºåºä¾æ¬¡ä¼ æ’­**
            idx = order[:, i]  # å–å½“å‰å±‚ç´¢å¼•

            for j in range(num_layers):  # **å®ƒå¯ä»¥å‘æ‰€æœ‰å±‚ä¼ æ’­**
                target_idx = order[:, j]  # ç›®æ ‡å±‚ç´¢å¼•
                if idx == target_idx:  # ğŸš¨ **è·³è¿‡è‡ªèº«**
                    continue

                weight = attn[:, idx, target_idx].unsqueeze(-1).unsqueeze(-1)
                if weight.mean() < self.tau:  # å‰ªæ
                    continue

                # ä¸Š/ä¸‹é‡‡æ ·è‡³ç›®æ ‡å±‚çš„å°ºåº¦
                scaled_feature = F.interpolate(new_features[idx], size=features[target_idx].shape[2:], mode='bilinear',
                                               align_corners=False)
                new_features[target_idx] += weight * scaled_feature  # ä¼ æ’­ç»™ç›®æ ‡å±‚

        # 2ï¸âƒ£ å¯¹æ¯ä¸ªå±‚çš„èåˆç‰¹å¾åº”ç”¨ 3x3 å·ç§¯è¿›è¡Œç»†åŒ–
        refined_features = [F.relu(self.fpn_convs[i](new_features[i])) for i in range(num_layers)]

        return refined_features
